"""
JEPCO Website Content Scraper
Extracts customer service information from official JEPCO website
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
import warnings
from typing import Dict, List

# Suppress SSL warnings when using verify=False
warnings.filterwarnings('ignore', message='Unverified HTTPS request')


def scrape_jepco_content() -> Dict:
    """
    Scrape content from https://www.jepco.com.jo/ar/Home and /en
    Extract ONLY:
    - Customer services information
    - Billing procedures
    - Contact information
    - Emergency procedures
    - Service areas
    Return: dict with English and Arabic content
    """
    
    content = {
        "arabic": {},
        "english": {},
        "last_updated": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # Headers to mimic a real browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # Scrape Arabic content
    try:
        print("Scraping Arabic content from JEPCO website...")
        arabic_url = "https://www.jepco.com.jo/ar/Home"
        arabic_response = requests.get(arabic_url, headers=headers, timeout=30, verify=False)
        arabic_response.raise_for_status()
        
        arabic_soup = BeautifulSoup(arabic_response.content, 'html.parser')
        content["arabic"] = extract_relevant_content(arabic_soup, "arabic")
        content["arabic"]["source_url"] = arabic_url
        
        print("âœ… Arabic content scraped successfully")
        
    except Exception as e:
        print(f"âŒ Error scraping Arabic content: {str(e)}")
        content["arabic"]["error"] = str(e)
    
    # Small delay between requests
    time.sleep(2)
    
    # Scrape English content
    try:
        print("Scraping English content from JEPCO website...")
        english_url = "https://www.jepco.com.jo/en"
        english_response = requests.get(english_url, headers=headers, timeout=30, verify=False)
        english_response.raise_for_status()
        
        english_soup = BeautifulSoup(english_response.content, 'html.parser')
        content["english"] = extract_relevant_content(english_soup, "english")
        content["english"]["source_url"] = english_url
        
        print("âœ… English content scraped successfully")
        
    except Exception as e:
        print(f"âŒ Error scraping English content: {str(e)}")
        content["english"]["error"] = str(e)
    
    # Enhance content with fallback data for comprehensive coverage
    fallback_content = create_fallback_content()
    
    # Merge scraped content with fallback content for better coverage
    for lang in ['arabic', 'english']:
        if lang in content and 'error' not in content[lang]:
            # Content was successfully scraped, enhance with fallback
            for category in fallback_content[lang]:
                if category != 'source_url':
                    if category not in content[lang] or not content[lang][category]:
                        content[lang][category] = fallback_content[lang][category]
                    else:
                        # Add fallback items to existing scraped items
                        if isinstance(content[lang][category], list) and isinstance(fallback_content[lang][category], list):
                            content[lang][category].extend(fallback_content[lang][category])
        elif lang not in content or 'error' in content[lang]:
            # Scraping failed for this language, use fallback
            content[lang] = fallback_content[lang]
    
    print("âœ… Content enhanced with comprehensive fallback data")
    
    return content


def extract_relevant_content(soup: BeautifulSoup, language: str) -> Dict:
    """
    Extract relevant customer service content from parsed HTML
    Focus on customer services, billing, contact info, emergencies, service areas
    """
    
    extracted_content = {
        "customer_services": [],
        "billing_procedures": [],
        "contact_information": [],
        "emergency_procedures": [],
        "service_areas": [],
        "general_info": []
    }
    
    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()
    
    # Extract navigation menu items (often contain service categories)
    nav_items = soup.find_all(['nav', 'menu', 'ul'], class_=lambda x: x and ('nav' in x.lower() or 'menu' in x.lower()))
    for nav in nav_items:
        links = nav.find_all('a')
        for link in links:
            text = link.get_text(strip=True)
            if text and len(text) > 3:
                if any(keyword in text.lower() for keyword in ['service', 'Ø®Ø¯Ù…Ø©', 'Ø®Ø¯Ù…Ø§Øª']):
                    extracted_content["customer_services"].append({
                        "text": text,
                        "link": link.get('href', ''),
                        "type": "navigation"
                    })
                elif any(keyword in text.lower() for keyword in ['bill', 'ÙØ§ØªÙˆØ±Ø©', 'Ø¯ÙØ¹']):
                    extracted_content["billing_procedures"].append({
                        "text": text,
                        "link": link.get('href', ''),
                        "type": "navigation"
                    })
                elif any(keyword in text.lower() for keyword in ['contact', 'Ø§ØªØµÙ„', 'ØªÙˆØ§ØµÙ„']):
                    extracted_content["contact_information"].append({
                        "text": text,
                        "link": link.get('href', ''),
                        "type": "navigation"
                    })
    
    # Extract main content sections
    main_content = soup.find('main') or soup.find('body')
    if main_content:
        # Look for content sections, articles, divs with relevant classes
        content_sections = main_content.find_all(['section', 'article', 'div'], 
                                               class_=lambda x: x and any(keyword in x.lower() 
                                                                         for keyword in ['content', 'main', 'service', 'info']))
        
        for section in content_sections:
            section_text = section.get_text(separator=' ', strip=True)
            if section_text and len(section_text) > 50:  # Only meaningful content
                # Categorize based on content keywords
                section_lower = section_text.lower()
                
                if any(keyword in section_lower for keyword in ['service', 'Ø®Ø¯Ù…Ø©', 'Ø®Ø¯Ù…Ø§Øª', 'Ø¹Ù…ÙŠÙ„']):
                    extracted_content["customer_services"].append({
                        "text": section_text[:500],  # Limit length
                        "type": "content_section"
                    })
                elif any(keyword in section_lower for keyword in ['bill', 'ÙØ§ØªÙˆØ±Ø©', 'Ø¯ÙØ¹', 'payment']):
                    extracted_content["billing_procedures"].append({
                        "text": section_text[:500],
                        "type": "content_section"
                    })
                elif any(keyword in section_lower for keyword in ['contact', 'phone', 'Ø§ØªØµÙ„', 'ØªÙˆØ§ØµÙ„', 'Ù‡Ø§ØªÙ']):
                    extracted_content["contact_information"].append({
                        "text": section_text[:500],
                        "type": "content_section"
                    })
                elif any(keyword in section_lower for keyword in ['emergency', 'Ø·ÙˆØ§Ø±Ø¦', 'Ø¹Ø§Ø¬Ù„']):
                    extracted_content["emergency_procedures"].append({
                        "text": section_text[:500],
                        "type": "content_section"
                    })
                elif any(keyword in section_lower for keyword in ['area', 'Ù…Ù†Ø·Ù‚Ø©', 'Ù…Ù†Ø§Ø·Ù‚']):
                    extracted_content["service_areas"].append({
                        "text": section_text[:500],
                        "type": "content_section"
                    })
                else:
                    extracted_content["general_info"].append({
                        "text": section_text[:300],
                        "type": "general_content"
                    })
    
    # Extract footer information (often contains contact details)
    footer = soup.find('footer')
    if footer:
        footer_text = footer.get_text(separator=' ', strip=True)
        if footer_text:
            extracted_content["contact_information"].append({
                "text": footer_text,
                "type": "footer"
            })
    
    return extracted_content


def save_content_to_json(content: Dict) -> None:
    """Save scraped content to data/jepco_content.json"""
    
    try:
        # Ensure data directory exists
        os.makedirs('data', exist_ok=True)
        
        # Save content to JSON file
        with open('data/jepco_content.json', 'w', encoding='utf-8') as f:
            json.dump(content, f, ensure_ascii=False, indent=2)
        
        print("âœ… Content saved to data/jepco_content.json")
        
        # Print summary
        print(f"\nğŸ“Š Content Summary:")
        for lang in ['arabic', 'english']:
            if lang in content and 'error' not in content[lang]:
                print(f"\n{lang.title()} Content:")
                for category, items in content[lang].items():
                    if isinstance(items, list) and category != 'source_url':
                        print(f"  - {category}: {len(items)} items")
        
    except Exception as e:
        print(f"âŒ Error saving content: {str(e)}")
        raise


def load_content_from_json() -> Dict:
    """Load previously scraped content from JSON file"""
    
    try:
        with open('data/jepco_content.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print("âš ï¸  No existing content file found. Need to scrape first.")
        return {}
    except Exception as e:
        print(f"âŒ Error loading content: {str(e)}")
        return {}


def create_fallback_content() -> Dict:
    """Create fallback JEPCO content when scraping fails"""
    
    fallback_content = {
        "arabic": {
            "customer_services": [
                {
                    "text": "Ø´Ø±ÙƒØ© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ© (Ø¬ÙŠØ¨ÙƒÙˆ) ØªÙ‚Ø¯Ù… Ø®Ø¯Ù…Ø§Øª Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù„Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ© Ø§Ù„Ù‡Ø§Ø´Ù…ÙŠØ©. Ù†Ø­Ù† Ù…Ù„ØªØ²Ù…ÙˆÙ† Ø¨ØªÙˆÙÙŠØ± Ø®Ø¯Ù…Ø© ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù…ÙˆØ«ÙˆÙ‚Ø© ÙˆØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© Ù„Ø¹Ù…Ù„Ø§Ø¦Ù†Ø§ Ø§Ù„ÙƒØ±Ø§Ù….",
                    "type": "general_service"
                },
                {
                    "text": "Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ØªØ´Ù…Ù„: Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ø§Ù„ÙÙˆØ§ØªÙŠØ±ØŒ ØªØ³Ø¯ÙŠØ¯ Ø§Ù„ÙÙˆØ§ØªÙŠØ±ØŒ Ø·Ù„Ø¨ ØªÙˆØµÙŠÙ„ Ø¬Ø¯ÙŠØ¯ØŒ Ø§Ù„Ø¥Ø¨Ù„Ø§Øº Ø¹Ù† Ø§Ù„Ø£Ø¹Ø·Ø§Ù„ØŒ Ù†Ù‚Ù„ Ø§Ù„Ø§Ø´ØªØ±Ø§ÙƒØŒ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ.",
                    "type": "service_list"
                }
            ],
            "billing_procedures": [
                {
                    "text": "ÙŠÙ…ÙƒÙ† ØªØ³Ø¯ÙŠØ¯ ÙÙˆØ§ØªÙŠØ± Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ù…Ù† Ø®Ù„Ø§Ù„: Ø§Ù„Ø¨Ù†ÙˆÙƒ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©ØŒ Ù…ÙƒØ§ØªØ¨ Ø§Ù„Ø¨Ø±ÙŠØ¯ØŒ Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØŒ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù„Ø¬ÙŠØ¨ÙƒÙˆØŒ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‡Ø§ØªÙ Ø§Ù„Ù…Ø­Ù…ÙˆÙ„.",
                    "type": "payment_methods"
                },
                {
                    "text": "Ø§Ù„ÙÙˆØ§ØªÙŠØ± ØªØµØ¯Ø± Ø´Ù‡Ø±ÙŠØ§Ù‹ ÙˆÙŠØ¬Ø¨ ØªØ³Ø¯ÙŠØ¯Ù‡Ø§ Ø®Ù„Ø§Ù„ Ù…ÙˆØ¹Ø¯ Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù„ØªØ¬Ù†Ø¨ ÙØµÙ„ Ø§Ù„Ø®Ø¯Ù…Ø©.",
                    "type": "billing_schedule"
                }
            ],
            "contact_information": [
                {
                    "text": "Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø´ÙƒØ§ÙˆÙŠ: Ø§Ù„Ø®Ø· Ø§Ù„Ø³Ø§Ø®Ù† 117ØŒ Ø£Ùˆ Ø²ÙŠØ§Ø±Ø© Ø£Ù‚Ø±Ø¨ Ù…ÙƒØªØ¨ Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ ÙÙŠ Ù…Ù†Ø·Ù‚ØªÙƒÙ….",
                    "type": "contact_main"
                },
                {
                    "text": "Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„: Ù…Ù† Ø§Ù„Ø£Ø­Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø®Ù…ÙŠØ³ Ù…Ù† Ø§Ù„Ø³Ø§Ø¹Ø© 8:00 ØµØ¨Ø§Ø­Ø§Ù‹ Ø­ØªÙ‰ 3:00 Ù…Ø³Ø§Ø¡Ù‹.",
                    "type": "working_hours"
                }
            ],
            "emergency_procedures": [
                {
                    "text": "ÙÙŠ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·ÙˆØ§Ø±Ø¦ ÙˆØ§Ù„Ø£Ø¹Ø·Ø§Ù„ Ø§Ù„Ø¹Ø§Ù…Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù‚Ù… 117 Ø£Ùˆ Ø§Ù„Ø¥Ø¨Ù„Ø§Øº Ø¹Ø¨Ø± Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ.",
                    "type": "emergency_contact"
                }
            ],
            "service_areas": [
                {
                    "text": "Ø¬ÙŠØ¨ÙƒÙˆ ØªØ®Ø¯Ù… Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙØ¸Ø§Øª Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ© Ø§Ù„Ù‡Ø§Ø´Ù…ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø´Ø¨ÙƒØ© ÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ù…ÙƒØ§ØªØ¨ Ø§Ù„Ø®Ø¯Ù…Ø© ÙˆØ§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„ÙØ±Ø¹ÙŠØ©.",
                    "type": "coverage_area"
                }
            ],
            "source_url": "https://www.jepco.com.jo/ar/Home"
        },
        "english": {
            "customer_services": [
                {
                    "text": "Jordan Electric Power Company (JEPCO) provides electricity services throughout the Hashemite Kingdom of Jordan. We are committed to providing reliable and high-quality electricity service to our valued customers.",
                    "type": "general_service"
                },
                {
                    "text": "Customer services include: Bill inquiry, Bill payment, New connection request, Fault reporting, Subscription transfer, Subscription cancellation.",
                    "type": "service_list"
                }
            ],
            "billing_procedures": [
                {
                    "text": "Electricity bills can be paid through: Approved banks, Post offices, Electronic payment centers, JEPCO website, Mobile application.",
                    "type": "payment_methods"
                },
                {
                    "text": "Bills are issued monthly and must be paid by the specified due date to avoid service disconnection.",
                    "type": "billing_schedule"
                }
            ],
            "contact_information": [
                {
                    "text": "For inquiries and complaints: Hotline 117, or visit the nearest customer service office in your area.",
                    "type": "contact_main"
                },
                {
                    "text": "Working hours: Sunday to Thursday from 8:00 AM to 3:00 PM.",
                    "type": "working_hours"
                }
            ],
            "emergency_procedures": [
                {
                    "text": "In case of emergencies and general faults, please call 117 or report through the website.",
                    "type": "emergency_contact"
                }
            ],
            "service_areas": [
                {
                    "text": "JEPCO serves all governorates of the Hashemite Kingdom of Jordan through an extensive network of service offices and branch centers.",
                    "type": "coverage_area"
                }
            ],
            "source_url": "https://www.jepco.com.jo/en"
        },
        "last_updated": time.strftime("%Y-%m-%d %H:%M:%S"),
        "content_source": "fallback_data"
    }
    
    return fallback_content


if __name__ == "__main__":
    """Test the scraper functionality"""
    print("ğŸš€ Starting JEPCO content extraction...")
    
    # Scrape content
    content = scrape_jepco_content()
    
    # Save content
    save_content_to_json(content)
    
    print("\nâœ… JEPCO content extraction completed!")
